--BASH--

--Для запуска из jupyter notebook
%%bash --перед кодом
! -- или в начале строки

%%bash

echo hello, bash

! echo hello, bash


Базовые команды
ls - просмотр текущей директории
ls *.html --Смотрим на файлы в текущей директории, которые подходят под паттерн *.html
ls -lh --списком с дополнительной информацией, дата, размер и т.д.


cd - изменение текущей директории --(названия файлов или папок с пробелами печатаем Мои/ документы)
pwd - просмотр пути до текущей директории
touch - создать пустой файл
mkdir - создать пустую директорию

cp - скопировать файл
cp -r #скопировать с вложенными директориями
cd ..  #  . означает текущюю директорию, .. - предыдущую директорию (на один уровень выше)

mv - переместить файл
echo - напечатать текст на экран
cat - напечатать содержимое файлов
which ls  -- Программа which показывает, где лежит программа ls
wc -l -- посчитать количество строк в файле

--Потоки данных--

Все эти три источника, как и все в UNIX, являются виртуальными файлами и выполняют следующие функции
* `stdin` представляет весь поток данных, который пользователь вводит с клавиатуры
* `stdout` представляет весь поток данных, который программа печатает на экран
* `stderr` представляет весь поток данных об ошибках в работе программы, который программа также печатает на экран

Все эти источники находятся по следующим путям: `/dev/stdout`, `/dev/stdin`, `/dev/stderr`. Можно заглянуть в директорию `/dev/` 
и посмотреть, сколько еще виртуальных источников данных есть в компьютере.

Оператор `>` позволяет перенаправлять стандартный вывод в любой другой файл.

Оператор `>>` позволяет не перезаписывать целиком файл, а лишь добавить в конец новые данные

Оператор `<` позволяет подменить стандартный ввод программы на другой файл.

оператором `<<<`. Он подаст на stdin ту строку, которую мы передадим в качестве аргумента.
--echo <<< "hello world"

Оператор `$(...)` - он берет вывод в stdout программы, которая описана внутри скобок и передает это значение в bash в виде обычной строки. 
! echo $(ls /etc/*.conf | head -n 3) # Печатаем три первых файла в директории /etc, которые подходят под паттерн *.conf
При использовании оператора `$` важно следить за тем, какие кавычки вы используете. Если использовать ординарные, то все содержимое будет считаться строкой. 
Если же использовать двойные, то оператор `$` внутри кавычек будет корректно подсчитан.
%%bash
echo 'Executable path for ls is $(which ls)'
echo "Executable path for ls is $(which ls)"
Отдельно стоит отметить, что одинарные скобки обрабатывают результат как строку, в то время как двойные скобки позволяют работать с арифметическими выражениями
%%bash

export hello=$(ls -l | wc -l)  # Строка
echo $hello
echo $(( $hello + 1 ))  # Арифметическое выражение
echo $(( $(ls -l | wc -l) / 3 ))  # Арифметическое выражение

Команда вида <(command1) работает следующим образом
* Создается временный виртуальный файл (такой же по сути, как /dev/stdin) и в него потоком льется stdout от команды command1
* Путь до этого виртуального файла передается виде строки обратно в баш для дальнейшего использования

Таким образом, комбинируюя операторы < и <() можно перенаправлять вывод одной команды в ввод другой.

%%bash

python3 interactive.py < <(yes) --yes специальная программа в BASH чтобы передовать согласие пользователя другим. командам

%%bash

diff <(ls /bin) <(ls /usr/bin) 
# diff выводит разницу между двумя файлам. Там мы можем узнать разницу между двумя директориями

--Pipes--
Идея соединять процессы через stdin\stdout очень популярна и для этого есть более удобный интерфейс - pipes.
Все команды, соединенные через | запускаются одновременно и общаются друг с другом через stdin\stdout

%%bash

echo hello | cat  # Печатаем hello и передаем его на stdin в cat
ls -l | cat | wc -l | python3 -c "print(int(input()) * 2)"
# Последняя программа выводит файлы в текущей директории, cat лишь перепечатываем эти данные
# Далее wc -l подсчитывает количество строк (к этой программе мы вернемся позже)
# И в самом конце программа на python считываем это количество, домножаем на два и выводит на экран

--Построение условных пайплайнов--
%%bash
echo foo
echo $?  # Печатаем статус-код. Так как echo foo завершился хорошо, то он будет равен 0

%%bash
cat unknown-file.txt
echo $?  # Так как cat не смог найти файл, то он завершился с ошибкой и поэтому статус-код равен 1

* `&&` Последующие команды будут выполнены, только если **успешно** завершились предыдущие
* `||` Последующие команды будут выполнены, только если **неуспешно** завершились предыдущие

Команду можно изолизовать в подкоманду, используя скобки. Таким образом можно например обходить ситуации, когда не так важно, успешно завершилась команда или нет.
! (rm non-exists-dir || exit 0) && echo PUPA  # echo PUPA всегда запустится

--Программы, при программировании на bash--

-head- 
читает определенное количество данных с начала файла. https://www.opennet.ru/man.shtml?topic=head&category=1
! head -n 2 /etc/hosts  # Читаем только первые 2 строки
! head -c 10 /etc/hosts  # Читаем только первые 10 байт

-Tail-
делает то же самое, что и `head`, но с конца файла https://www.opennet.ru/man.shtml?topic=tail&category=1

! tail -n 2 /etc/hosts #две последние строки
! tail -n +2 /etc/hosts  # все строки после второй строки (включая вторую строку)

-Sort-
сортирует входные данные. По умолчанию используется лексикографический порядок, но это поведение можно поменять с использованием специальных опций.
https://www.opennet.ru/man.shtml?topic=sort&category=1

! sort example.txt -n  # Сортируем как числа
! cat example.txt | sort -n  # Можно подавать на вход данные через stdin

-Shuf-
случайным образом перемешивает входящие данные
! cat example.txt | shuf

-Uniq-
Оставляет только уникальные значения. Однако он корректно работает только с отсортированными данными. Для этого мы можем предварительно использовать `sort`.
Помимо операции схлопывания одинаковых значений, `uniq` также умеет считать простые статистики для схлопнувшихся групп. Этот функционал чем-то напоминает 
group by. Так, ключ `-c` считает количество элементов каждой группе.
https://www.opennet.ru/man.shtml?topic=uniq&category=1&russian=0

! cat example.txt | sort | uniq
! cat example.txt | sort | uniq -c   # подсчет каждого символа в файле

-Wc-
Cчитает количество элементов во входных данных. По умолчанию считает три характеристики - количество строк, количество слов, количество байт. 
Различные опции позволяют считать какую-то одну из характеристик. Например `-l` считает количество строк в данных.
https://www.opennet.ru/man.shtml?topic=wc&category=1&russian=0

-cut-
парсит строки, которые состоят из значений с разделителем. С помощью утилиты можно обрабатывать различные регулярные форматы данных, 
базирующиеся на разделителях. Например `csv` или `tsv`.
https://www.opennet.ru/man.shtml?topic=cut&category=1&russian=0
https://www.youtube.com/watch?v=_GxXbM0METw&t=3

! head cities.csv | cut -d',' -f6,7  # разделяем данные по запятой и берем только 6 и 7 столбец
! cat cities.csv | tail -n +2 | cut -d',' -f9 | head  # смотрим только колонку City. Tail нужен, чтобы убрать заголовок
! cat cities.csv | tail -n +2 | cut -d',' -f9 | sort | uniq | wc -l  # считаем количество уникальных городов 
! cat res.csv |cut -d',' -f1- #столбцы с 1 до конца

-grep-
позволяет фильтровать входной поток по указанному регулярному выражению
https://www.opennet.ru/man.shtml?topic=grep&category=1

! cat cities.csv | grep ton  # ищем строки, где присутствует ton
! ls | grep "^[0-9]"  # Файлы, которые начинаются с числа

Можно использовать и более сложные регулярные выражения для поиска
Подробнее о том, как составлять паттерны и какие еще возможности есть у grep - https://www.gnu.org/software/grep/manual/grep.html


-ps-
(process status) показывает информацию о запущенных процессах в системе. У команды есть ряд параметров, которые задают формат отображения. 
Наиболее популярная комбинация для отслеживания состояния системы - `ps aux`

* `a` - Показать все процессы для всех пользователей
* `x` - Показывать процессы, которые отсоеденины от терминала (работают в фоне)
* `u` - Показывать информацию о пользователе, связанного с конкретным процессов

! ps aux | head  # смотрим информацию по запущенным процессам
! ps aux | grep python  # смотрим все процессы питона

-sed-
(sequence editor) - редактор входного потока. Имеет свой не очень сложный формат задания правил редактирования.
Возможные команды:
* `[диапазон]p` - напечатать диапазон строк
* `[диапазон]d` - удалить диапазон строк
* `s/[паттерн1]/[паттерн2]` - заменить строку, подходящую под паттерн1 на паттерн2. Может комбинироваться с диапазоном. Также может добавляться ключ 
`g` (`s/[паттерн1]/[паттерн2]/g`) , позволяющая применять правила ко всему потоку сразу. Без этой опции ищется только первое совпадние с паттерном.
https://www.opennet.ru/docs/RUS/bash_scripting_guide/a14586.html

Команды для sed можно указывать в файле, однако если сами команды не слишком большие, то удобнее их указывать в виде аргументов командной строки. 
Для этого необходимо использовать флаг `-e`.

Также при использовании оператора `p`, необходимо указывать `-n`, чтобы не отображать лишние строки. В противном случае sed будет выводить вообще 
все исходные данные.

! cat cities.csv | sed -n -e "3"p  # печатаем третью строку
! cat cities.csv | sed -n -e "0~3"p | head # печатаем каждую третью строку, начиная с нулевой
! cat cities.csv | sed -n -e "3,5"p  # печатаем с 3 по 5 строку
! cat cities.csv | sed -n -e "6,$"p | head  # печатаем с 6 до конца
! cat cities.csv | sed -n -e "/ton/"p  # печатаем все, которые подходят под паттерн
# Паттерны могут быть и более сложными. Например здесь паттерн ищем все Yankton ИЛИ NC
! cat cities.csv | sed -n -e "/Yankton\|NC/"p
! cat cities.csv | sed -n -e "/Yankton/,/Wilmington/"p  # поток от первой записи с Yankton до Wilmington
! cat cities.csv | sed -e "3"d | head # удаляем третью строку (то есть печатаем все, кроме этой строки)
! cat cities.csv | sed -e "6,$"d  # удаляем все после 6
! cat cities.csv | sed -e "/Yankton/,/Wilmington/"d | head # удаляем все с Yankton до Wilmington
! head cities.csv | sed -e "s/Youngstown/Pupatown/"
! ps aux | grep python | sed -e "s/\s\+/,/g"  # заменяем все пробельные символы (\s) которые идут подряд (\+) на ,
! ps aux | grep python | sed -e "s/\s\+/,/g" | cut -d',' -f2  # получаем pid всех процессов

Команда `sed` также имеет ключ -i позволяющий редактировать файлы "на месте". Например мы можем убрать заголовок у csv файла, не создавая новый, а отредактировав старый, 
что гораздо быстрее и менее затратно с точки зрения диска.

! sed -i cities-2.csv -e '1'd #убрать первую строку в файле

! cat cities.csv | head -n 1 | cut -d',' -f9,10 | sed -e 's|["'\'']||g' >> result.csv #берем только, берем только 9 и 10 столбцы, заменяем удалеяем 
одинарные и двойные кавычки, через замену
! cat cities.csv | cut -d',' -f9,10 | sed -n -e "/San\|Saint/"p | sed -e 's|["'\'']||g' >> result.csv #тоже самое плюс отбираем только строки которые содержат 
San или Saint


-awk-
как и `sed`, это редактор входного потока данных. В отличии от `sed` он крайне продвинутый и имеет собственный язык инструкций. Разбирать целиком этот язык 
мы не будем - продемонстрируем лишь некоторые примеры того, какие операции с ним можно проводить.
https://www.opennet.ru/docs/RUS/awk/

! cat cities.csv | awk -F, '{print "City = " $9 " State = " $10}' | head # -F указывает на , как на разделитель.
# Далее командой print мы печатаем собранную строку с 9-м и 10-м столбкцом

! cat cities.csv | tail -n +2 | awk -F, 'BEGIN {print "city,state"} {print $9 "," $10}'  | head
# BEGIN указывает на инструкцию, которую нужно выполнить один раз в самом начале. В нашем случае это заголовок csv
# Таким образом мы сделали новую csv таблицу, в которую вклчили только столбцы city и state

! cat cities.csv | awk -F, '{ if ($5 > 100) print $5, $9, $10 }'  # все записи, где NS больше 100

! cat cities.csv | awk -F, '{SUM += $5} END {print SUM}'
# Суммируем NS (пятая колонка). END указывает на то, что последнюю конструкцию нужно запустить 1 раз в конце.
# В нашем случае это вывести итоговую сумму

# Суммируем третью колонку
! df | tail -n +2 | sed -e 's/\s\+/,/g' | awk -F, '{SUM += $3} END {print SUM}'

-df-
показать статистику использование места на диске

-jq-
Yе является стандартной программой и ее необходимо самостоятельно установить. Для Ubuntu - `apt-get install jq`. В этом окружении jq уже установлена.
`jq` - это манипулятор JSON документами. Имеет свой язык запросов к JSON, чем то похожий на пайплайны в bash. 
https://stedolan.github.io/jq/

! cat covid.json | jq '.measures[0]' #посмотреть первый элемент массива measures
! cat covid.json | jq '.measures[0].country' #вывести значение country из первого элемента массива measures
! cat covid.json | jq '. | keys' # вывести список элементов из корневого каталога
! cat covid.json | jq '.measures[0] | keys' вывести список ключей для массива measures
! cat covid.json | jq '.measures[3:5]' #Вместо конкретного элемента можно взять срез или целый массив сразу.
! cat covid.json | jq '.measures[]' | head -n 100   # все элементы массива. 
! cat covid.json | jq '.measures | length'  # В массиве 1000 элементов
! cat covid.json | jq '.measures[0:200] | .[].confirmed'
! cat covid.json | jq '.measures | [{"country_name": .[].country}] | .[:10]' #Из потоков можно конструировать другой json
! cat covid.json | jq '.measures[].country' | sort | uniq  #С помощью jq можно фильтровать запросы примерно таким же образом как в MongoDB. 
Для этого есть оператор select
! cat covid.json | jq -r '.measures[].country' | sort | uniq  #Чтобы печатать строковые значения без кавычек можно использовать ключ `-r`
! cat covid.json | jq '.measures[] | select(.country == "Angola")' | head -n 50
! cat covid.json | jq '.measures[] | select(.country == "Angola") | .confirmed'
! cat covid.json | jq '.measures | unique_by(.country)' #Для использования также доступны функции по сортировке и нахождению уникальных элементов. 
Обе функции работают с массивом.
! cat covid.json | jq '.measures | unique_by(.country) | sort_by(.population) | reverse'

--Tar & Zip--
Очень часто данные хранятся в виде архивов. Команды `tar` и `zip\unzip` позволяют распаковывать архивы.

`tar` имеет целый набор однобуквенных ключей, комбинация которых позволяет производить различные операции над архивами.
* `c` - создать архив
* `x` - распаковать архив
* `z` - использовать алгоритм gzip. Архивы, созданные с таким алгоритмом, имеют расширение `.tar.gz`
* `v` - печатать на экран детали распаковки
* `f` - считать архив из указанного файла

`zip\unzip` работает немного проще. Команда `zip` создает новый архив, команда `unzip` распаковывает указанный архив.
https://www.opennet.ru/man.shtml?topic=tar&category=1 
https://www.opennet.ru/man.shtml?topic=unzip&category=1&russian=4

! tar -czvf cities.tar.gz cities.csv
! mkdir -p tar-cities && tar -xzvf cities.tar.gz -C tar-cities/ # Распакуем этот архив в новую директорию tar-cities

! zip cities.zip cities.csv
! mkdir -p zip-cities && yes | unzip cities.zip -d zip-cities/

--Wget & Curl--
`wget` и `curl` позволяют выгружать данные из интернета.
`wget` более продвинутый - он умеет скачивать сразу множество файлов, поддерживает докачку файлов и так далее.
`curl` более простой и может использоваться скорее для точечных запросов.
https://www.opennet.ru/man.shtml?topic=wget&category=1&russian=0 
https://www.opennet.ru/man.shtml?topic=curl&category=1&russian=3

! curl -L http://localhost:8888/notebooks > notebooks-curl.html
! wget http://localhost:8888/notebooks -O notebooks-wget.html

%%writefile link-list.txt
http://localhost:8888
http://localhost:8888/notebooks

Создатим список ссылок и скачаем их все разом с помощью ключа `-i`
! wget -i link-list.txt

--Пример обработки данных--

! cat aact.csv | tail -n +2 | cut -d"," -f3 | sort | uniq #* Отрезаем заголовок, Разбиваем по запятой, Сортируем и считаем уникальные
! cat aact.csv | wc -l #количество строк в файле
! cat aact.csv | tail -n +2 | sed -e "/N\/A/"d | wc -l #считаем количество строк с N\A
! cat aact.csv | tail -n +2 | sed -e "/N\/A/"d | cut -d"," -f3 | sort | uniq #Уберем N\A значения - не хотим их видеть в выборке
! cat aact.csv | tail -n +2 | sed -e "/N\/A/"d > aact-fillna.csv #Отлично, сохраним этот результат в промежуточных файл

Предположим, что мы хотим составить набор данных для алгоритма машинного обучения, который бы предсказывал тип исследования по его описанию.
Для этого нам необходимо оставить только текст и значение study_type, перемешать все данные и разделить их на два набора данных - один train для обучения машины,
другой test для проверки корректности работы в соотношении 70\30.

%%bash

export TRAIN_SIZE=$(( $(cat aact-fillna.csv | wc -l) * 70 / 100 ))
echo $TRAIN_SIZE

cat aact-fillna.csv | cut -d"," -f3,6,7 | shuf > shuffled-aact.csv

cat shuffled-aact.csv | head -n $TRAIN_SIZE > train-aact.csv
cat shuffled-aact.csv | tail -n +$(($TRAIN_SIZE+1)) > test-aact.csv

! cat aact.csv | sort -t ',' -k2 | head  # сортируем по nct id
#Команда `sort` также имеет возможность сортировать поток не по целой строке, а по определенной ее части.
Ключ `-t` указывает разделитель строки , а ключ `-k` указывает номер секции (колонки) по которой необходимо сортировать. 

--Пример обработки даннных в JSON--

! cat covid.json | jq '.measures[].confirmed' | awk '{sum += $1} END {print sum}' #считаем количество зараженных
! cat covid.json | jq '.measures[].confirmed' | awk '{sum += $1; count += 1} END { print sum / count }' считаем среднее

#Считаем медиану
%%bash

export MEDIAN_INDEX=$(( $(cat covid.json | jq '.measures[].confirmed' | wc -l) / 2 ))
echo $MEDIAN_INDEX

cat covid.json | jq '.measures[].confirmed' | sort -n | sed -n -e "$MEDIAN_INDEX"p

--Word count--
Подсчитать количество раз, которое встречается каждое слово в документе. В качестве самого документа со словами будем использовать заголовки из aact

! cat aact.csv | tail -n +2 | cut -d"," -f6,7 | head  # получаем только тексты
! cat aact.csv | tail -n +2 | cut -d"," -f6,7 | sed -e "s/\s\+/\\n/g"  | head  # заменяем все пробелы на перенос строк
! cat aact.csv | tail -n +2 | cut -d"," -f6,7 | sed -e "s/\s\+/\\n/g" | sort | uniq -c # сортируем и схлопываем
! cat aact.csv | tail -n +2 | cut -d"," -f6,7 | sed -e "s/\s\+/\\n/g" | sort | uniq -c > word-count.txt #Теперь хотим узнать топ 20 самых популярных слов в тексте
# -k1 = первое поле, n = сравниваем как числа, r - в обратном порядке
! cat word-count.txt | sort -t ' ' -k1nr | head -n 20  
